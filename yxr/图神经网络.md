---
title: 图神经网络
description: 
published: true
date: 2023-12-26T03:09:48.242Z
tags: 
editor: markdown
dateCreated: 2023-12-24T12:31:49.575Z
---

# 图神经网络

## A Comprehensive Survey on Graph Neural Networks

> 尽管传统的深度学习方法被应用在提取欧氏空间数据的特征方面取得了巨大的成功，但许多实际应用场景中的数据是从非欧式空间生成的，传统的深度学习方法在处理非欧式空间数据上的表现却仍难以使人满意。例如，在电子商务中，一个基于图（Graph）的学习系统能够利用用户和产品之间的交互来做出非常准确的推荐，但图的复杂性使得现有的深度学习算法在处理时面临着巨大的挑战。这是因为图是不规则的，每个图都有一个大小可变的无序节点，图中的每个节点都有不同数量的相邻节点，导致一些重要的操作（例如卷积）在图像（Image）上很容易计算，但不再适合直接用于图。此外，现有深度学习算法的一个核心假设是数据样本之间彼此独立。然而，对于图来说，情况并非如此，图中的每个数据样本（节点）都会有边与图中其他实数据样本（节点）相关，这些信息可用于捕获实例之间的相互依赖关系。

图神经网络可以分为四类：

1.循环图神经网络
2.卷积图神经网络
3.图自编码器
4.时空图神经网络


## 2.Hypergraph Learning: Methods and Practices

### 2.1超图构建
#### 2.1.1基于距离构建超图

一般使用**最邻近搜索**或**聚类**构建，在特征空间中连接相似的顶点，或者位置距离比较近的顶点。超边的权重一般计算为：

$$
w(e) = \sum\limits_{u,v \in e} exp( -\frac{d(\mathbf{X}(u),\mathbf{X}(v))^{2}}{\sigma^{2}})
$$

$u,v$表示超边$e$中的一对顶点，$\sigma$是所有顶点距离的中值。

> 基于特征空间的距离构建超图可能导致不准确，因为噪声或离群值。而且连接多少邻居也是一个重要的问题，自适应学习这个值并不trivial。

#### 2.1.2基于表示构建超图

#### 2.1.3基于属性构建超图
利用属性构造超边，如图所示：

<img src='/hypergraph/属性超图构建.png' align="middle" width=40%>

这里的属性也许可以为光场光谱的空间属性$(x,y)$，角度属性$(u,v)$，光谱属性$\lambda_{u,v}$。

每个超边都可被看做一个团(clique)，这个团中的平均热核权重可被表示为：

$$
w(e)=\frac{1}{\delta(e)(\delta(e)-1)} \sum\limits_{u,v \in e}exp( -\frac{||\mathbf{X}(u)-\mathbf{X}(v)||_{2}^{2}}{\sigma^{2}} )
$$

其中，$\delta(e)$表示超边$e$的度。

> 虽然属性信息在数据表示方面具有显著优势，可以表示不同属性的层次，但在某些情况下，这些信息可能不可用。一种可能的解决方案是定义一组可以从现有数据中学习的属性[1]。
[1] Y. Fang and Y. Zheng, “Metric learning based on attribute hypergraph,” in Proc. IEEE Int. Conf. Image Process., 2017, pp. 3440–


#### 2.1.4深度学习构建超图


### 2.2超图学习
#### 2.2.1标签预测
构建超图后，可以进行对未标记顶点的标签进行预测、动态结构更新、多模态学习等任务。

给定一个超图$\mathcal{G}=(\mathcal{V},\mathcal{E},\mathbf{W})$，训练顶点就是带标签的顶点，标签预测任务是预测剩余未标记顶点的标签。设$\mathbf{F}(v) \in \mathbb{R}^{|L|}$表示分类置信度函数，返回向量的第$i$个元素为该顶点属于标签$i$的置信度。超图学习模型可以用两部分构建目标函数：

$$
\argmin\limits_{\mathbf{F}} \mathbf{\Psi}(\mathbf{F}) := {\mathbf{\Omega}(\mathbf{F}) + \lambda R_{emp}(\mathbf{F})} 
$$

其中，$\lambda$是平衡参数，$R_{emp}(\mathbf{F})$是经验损失函数，$\Omega(\mathbf{F})$是正则项。

#### 2.2.2动态超图结构学习

与静态超图不同，超图的动态结构更新是在学习过程中动态调整超图组件，包括超边权值、顶点权值和关联矩阵，是数据关联建模更加准确。这个问题对超图学习非常重要，因为数据关联建模强烈依赖于超图结构的质量。

#### 2.2.3从多模态数据中学习超图
> 在转导学习中，训练和测试阶段都是在线的。即所有的训练数据都是在线的。在线学习过程中的计算成本很高。与转导学习模式不同，归纳学习包含离线训练和在线分类过程。分类函数$\mathbf{F}$被定义为从原始特征$\mathbf{X}$到标签向量的投影，即$\mathbf{F} = \mathbf{X}^{T} \mathbf{M}$。

##### 2.2.3.1多模转导超图学习

以光场光谱数据为例，有m个（3个或5个）模态，对于每个模态，我们可以将每个主体视作顶点来生成并构造一个超图。这样可以构造m个超图$\mathcal{G}_{1}=(\mathcal{V}_{1},\mathcal{E}_{1},\mathbf{W}_{1}),\mathcal{G}_{2}=(\mathcal{V}_{2},\mathcal{E}_{2},\mathbf{W}_{2}),\cdots,\mathcal{G}_{m}=(\mathcal{V}_{m},\mathcal{E}_{m},\mathbf{W}_{m})$。为了保证全局有效性，一个对应于超图$\mathcal{G}_{i}$的优化权重$\alpha_{i}$被引入学习顶点label向量。学习目标函数可以被写作：
$$
\underset{\mathbf{F}, \alpha}{\operatorname{argmin}} \Psi(\mathbf{F}, \alpha):=\left\{\Omega(\mathbf{F}, \alpha)+\lambda \mathcal{R}_{e m p}(\mathbf{F})+\mu \Phi(\alpha)\right\}
$$
其中，$\Phi(\alpha)$表示超图权重约束。这里的$\mathbf{F}$可以重新看做$\mathbf{F}(v) \in \mathbb{R}^{|\lambda|}$，表示顶点$v$的全光谱特征。
$$
\Omega(\mathbf{F}, \alpha)=\mathbf{F}^T \sum_{i=1}^m \alpha_i\left(\mathbf{I}-\Theta_i\right) \mathbf{F}
$$
其中，$\boldsymbol{\Theta}_i=\mathbf{D}_{v i}^{-1 / 2} \mathbf{H}_i \mathbf{W}_i \mathbf{D}_{e i}^{-1} \mathbf{H}_i^T \mathbf{D}_{v i}^{-1 / 2}$。注意到超图权重之和为1，公式可以进一步写作：
$$
\begin{aligned}
& \underset{\mathbf{F}, \alpha}{\operatorname{argmin}} \Psi(\mathbf{F}, \alpha):= \\
& \left\{\mathbf{F}^T \sum_{i=1}^m \alpha_i\left(\mathbf{I}-\Theta_i\right) \mathbf{F}+\lambda\|\mathbf{F}-\mathbf{Y}\|^2+\mu \sum_{i=1}^m \alpha_i^2\right\}, \\
& \text { s.t. } \sum_{i=1}^m \alpha_i=1 .
\end{aligned}
$$

##### 2.2.3.2多模归纳超图学习


